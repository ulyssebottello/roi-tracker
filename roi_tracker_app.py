"""
ROI Tracker Chatbot - Outil d'analyse ROI pour chatbots
Technologie: Python + Streamlit
Objectif: Analyser les comportements track√©s sur un site web pour calculer le ROI d'une solution chatbot
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import re
from collections import Counter

# Configuration de la page
st.set_page_config(
    page_title="ROI Tracker - Analyse Chatbot",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# CSS personnalis√© pour l'interface
st.markdown("""
<style>
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        text-align: center;
    }
    .section-header {
        font-size: 1.5rem;
        font-weight: bold;
        margin: 2rem 0 1rem 0;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# Fonction utilitaire pour le formatage des euros avec s√©parateurs de milliers
def format_euro(amount: float) -> str:
    """Formate un montant en euros avec s√©parateurs de milliers (espaces)"""
    return f"{amount:,.2f}‚Ç¨".replace(',', ' ')

# Classes pour la gestion des donn√©es
class ROIDataProcessor:
    """Classe pour traiter et analyser les donn√©es ROI"""
    
    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()
        self.processed_data = None
        self.consolidated_data = None
        self.events_analysis = {}
        self.filtered_data = None
        self.date_filter_applied = False
        
    def validate_data_structure(self) -> Tuple[bool, str]:
        """Valide la structure du DataFrame"""
        # Support de deux formats de donn√©es
        # Format 1: name, values, date (format cl√©:valeur)
        # Format 2: name, date + colonnes value_* (format colonnes s√©par√©es)
        
        required_base = ['name', 'date']
        
        if not all(col in self.df.columns for col in required_base):
            missing_cols = [col for col in required_base if col not in self.df.columns]
            return False, f"Colonnes de base manquantes: {missing_cols}"
        
        # V√©rification du format
        has_values_column = 'values' in self.df.columns
        has_value_columns = any(col.startswith('value_') for col in self.df.columns)
        
        if not has_values_column and not has_value_columns:
            return False, "Aucune colonne de valeurs trouv√©e (attendu: 'values' ou 'value_*')"
        
        if self.df.empty:
            return False, "Le fichier est vide"
            
        return True, "Structure valide"
    
    def parse_values_column(self) -> pd.DataFrame:
        """Parse les colonnes de valeurs selon le format d√©tect√©"""
        consolidated_data = {}
        
        # D√©tection du format de donn√©es
        has_values_column = 'values' in self.df.columns
        has_value_columns = any(col.startswith('value_') for col in self.df.columns)
        
        for idx, row in self.df.iterrows():
            # Cl√© unique pour identifier un √©v√©nement (nom + date + conversation)
            event_key = (
                row['name'], 
                row['date'], 
                row.get('conversation_id', row.get('conversationId', ''))
            )
            
            # Initialiser l'√©v√©nement s'il n'existe pas
            if event_key not in consolidated_data:
                consolidated_data[event_key] = {
                    'name': row['name'],
                    'date': row['date'],
                    'conversation_id': row.get('conversation_id', row.get('conversationId', '')),
                    'values': {}
                }
            
            if has_values_column:
                # Format 1: colonne 'values' avec format cl√©:valeur
                values_str = str(row['values'])
                
                # Parse le format cl√©:valeur
                if ':' in values_str:
                    parts = values_str.split(':', 1)
                    if len(parts) == 2:
                        key = parts[0].strip()
                        value = parts[1].strip()
                        
                        # Tentative de conversion en nombre pour les prix
                        numeric_value = None
                        try:
                            numeric_value = float(value)
                        except ValueError:
                            pass
                        
                        consolidated_data[event_key]['values'][key] = {
                            'text': value,
                            'numeric': numeric_value
                        }
                        
            elif has_value_columns:
                # Format 2: colonnes value_* s√©par√©es
                value_columns = [col for col in self.df.columns if col.startswith('value_')]
                
                has_values = False
                
                for value_col in value_columns:
                    value_content = row[value_col]
                    
                    # Si la valeur est vide/NaN, continuer
                    if pd.isna(value_content) or str(value_content).strip() == '':
                        continue
                    
                    has_values = True
                    
                    # Extraction de la cl√© depuis le nom de colonne (value_custom -> custom)
                    key_from_column = value_col.replace('value_', '')
                    
                    # Parse du contenu - gestion des multiples paires s√©par√©es par |
                    content_str = str(value_content).strip()
                    
                    # V√©rifier s'il y a des multiples paires s√©par√©es par |
                    if '|' in content_str and ':' in content_str:
                        # Format multiple : cl√©1:valeur1|cl√©2:valeur2|...
                        pairs = content_str.split('|')
                        for pair in pairs:
                            pair = pair.strip()
                            if ':' in pair:
                                parts = pair.split(':', 1)
                                if len(parts) == 2:
                                    key = parts[0].strip()
                                    value = parts[1].strip()
                                    
                                    # Tentative de conversion en nombre pour les prix
                                    numeric_value = None
                                    try:
                                        # Nettoyer la valeur pour extraction num√©rique (enlever ‚Ç¨, espaces, etc.)
                                        clean_value = value.replace('‚Ç¨', '').replace(',', '.').strip()
                                        numeric_value = float(clean_value)
                                    except ValueError:
                                        pass
                                    
                                    consolidated_data[event_key]['values'][key] = {
                                        'text': value,
                                        'numeric': numeric_value
                                    }
                    elif ':' in content_str:
                        # Format simple : cl√©:valeur
                        parts = content_str.split(':', 1)
                        if len(parts) == 2:
                            key = parts[0].strip()
                            value = parts[1].strip()
                        else:
                            key = key_from_column
                            value = content_str
                    else:
                        # Pas de format cl√©:valeur, utiliser le nom de colonne comme cl√©
                        key = key_from_column
                        value = content_str
                    
                    # Si on est dans les cas simples, traiter la valeur
                    if not ('|' in content_str and ':' in content_str):
                        # Tentative de conversion en nombre pour les prix
                        numeric_value = None
                        try:
                            # Nettoyer la valeur pour extraction num√©rique (enlever ‚Ç¨, espaces, etc.)
                            clean_value = value.replace('‚Ç¨', '').replace(',', '.').strip()
                            numeric_value = float(clean_value)
                        except ValueError:
                            pass
                        
                        consolidated_data[event_key]['values'][key] = {
                            'text': value,
                            'numeric': numeric_value
                        }
                
                # Si aucune valeur n'a √©t√© trouv√©e, marquer comme no_value
                if not has_values:
                    consolidated_data[event_key]['values']['no_value'] = {
                        'text': '',
                        'numeric': None
                    }
        
        # Conversion en format DataFrame consolid√©
        final_data = []
        for event_key, event_data in consolidated_data.items():
            # Cr√©er une ligne consolid√©e avec toutes les valeurs
            row_data = {
                'name': event_data['name'],
                'date': event_data['date'],
                'conversation_id': event_data['conversation_id']
            }
            
            # Ajouter toutes les valeurs comme colonnes s√©par√©es
            for value_key, value_info in event_data['values'].items():
                row_data[f'{value_key}_text'] = value_info['text']
                row_data[f'{value_key}_numeric'] = value_info['numeric']
            
            final_data.append(row_data)
        
        consolidated_df = pd.DataFrame(final_data)
        
        # Cr√©er √©galement un format "long" pour compatibilit√© avec l'analyse existante
        long_format_data = []
        for event_key, event_data in consolidated_data.items():
            for value_key, value_info in event_data['values'].items():
                long_format_data.append({
                    'name': event_data['name'],
                    'date': event_data['date'],
                    'value_key': value_key,
                    'value_text': value_info['text'],
                    'value_numeric': value_info['numeric'],
                    'conversation_id': event_data['conversation_id']
                })
        
        # Stocker les deux formats
        self.consolidated_data = consolidated_df
        return pd.DataFrame(long_format_data)
    
    def detect_value_types(self) -> Dict[str, str]:
        """D√©tecte automatiquement les types de valeurs"""
        if self.processed_data is None:
            self.processed_data = self.parse_values_column()
            
        type_mapping = {}
        
        for key in self.processed_data['value_key'].unique():
            key_lower = key.lower()
            if 'price' in key_lower or 'prix' in key_lower:
                type_mapping[key] = 'price'
            elif 'product' in key_lower or 'produit' in key_lower:
                type_mapping[key] = 'product'
            else:
                type_mapping[key] = 'custom'
                
        return type_mapping
    
    def get_date_range(self) -> Tuple[datetime, datetime]:
        """Retourne la plage de dates disponible dans les donn√©es"""
        if self.processed_data is None:
            self.processed_data = self.parse_values_column()
            
        # Conversion de la colonne date
        self.processed_data['date'] = pd.to_datetime(self.processed_data['date'], errors='coerce')
        
        # Normaliser les timezones pour √©viter les erreurs de comparaison
        if self.processed_data['date'].dt.tz is not None:
            self.processed_data['date'] = self.processed_data['date'].dt.tz_convert(None)
        
        min_date = self.processed_data['date'].min()
        max_date = self.processed_data['date'].max()
        
        return min_date, max_date
    
    def apply_date_filter(self, start_date: datetime, end_date: datetime):
        """Applique un filtre de date aux donn√©es"""
        if self.processed_data is None:
            self.processed_data = self.parse_values_column()
            
        # Conversion de la colonne date si pas d√©j√† fait
        self.processed_data['date'] = pd.to_datetime(self.processed_data['date'], errors='coerce')
        
        # Normaliser les timezones pour √©viter les erreurs de comparaison
        # Convertir en timezone naive si les dates ont une timezone
        if self.processed_data['date'].dt.tz is not None:
            self.processed_data['date'] = self.processed_data['date'].dt.tz_convert(None)
        
        # S'assurer que les dates de filtrage sont aussi timezone naive
        if start_date.tzinfo is not None:
            start_date = start_date.replace(tzinfo=None)
        if end_date.tzinfo is not None:
            end_date = end_date.replace(tzinfo=None)
        
        # Filtrer les donn√©es selon la p√©riode
        mask = (
            (self.processed_data['date'] >= start_date) & 
            (self.processed_data['date'] <= end_date)
        )
        
        self.filtered_data = self.processed_data[mask].copy()
        self.date_filter_applied = True
        
        # Filtrer aussi les donn√©es consolid√©es si elles existent
        if self.consolidated_data is not None:
            consolidated_dates = pd.to_datetime(self.consolidated_data['date'], errors='coerce')
            
            # Normaliser les timezones pour les donn√©es consolid√©es aussi
            if consolidated_dates.dt.tz is not None:
                consolidated_dates = consolidated_dates.dt.tz_convert(None)
                
            consolidated_mask = (
                (consolidated_dates >= start_date) & 
                (consolidated_dates <= end_date)
            )
            self.consolidated_data = self.consolidated_data[consolidated_mask].copy()
    
    def get_active_data(self) -> pd.DataFrame:
        """Retourne les donn√©es actives (filtr√©es ou compl√®tes)"""
        if self.date_filter_applied and self.filtered_data is not None:
            return self.filtered_data
        else:
            if self.processed_data is None:
                self.processed_data = self.parse_values_column()
            return self.processed_data
    
    def get_global_metrics(self) -> Dict:
        """Calcule les m√©triques globales"""
        # Utiliser les donn√©es actives (filtr√©es ou compl√®tes)
        active_data = self.get_active_data()
        
        # Conversion de la colonne date
        active_data['date'] = pd.to_datetime(active_data['date'], errors='coerce')
        
        # Normaliser les timezones pour √©viter les erreurs de comparaison
        if active_data['date'].dt.tz is not None:
            active_data['date'] = active_data['date'].dt.tz_convert(None)
        
        total_events = len(active_data)
        unique_events = active_data['name'].nunique()
        date_range = (
            active_data['date'].min(),
            active_data['date'].max()
        )
        
        # R√©partition par type d'√©v√©nement
        event_distribution = active_data['name'].value_counts()
        
        # R√©partition par type de valeur
        value_type_distribution = active_data['value_key'].value_counts()
        
        return {
            'total_events': total_events,
            'unique_events': unique_events,
            'date_range': date_range,
            'event_distribution': event_distribution,
            'value_type_distribution': value_type_distribution
        }
    
    def analyze_selected_events(self, selected_events: List[str]) -> Dict:
        """Analyse les √©v√©nements s√©lectionn√©s"""
        # Utiliser les donn√©es actives (filtr√©es ou compl√®tes)
        active_data = self.get_active_data()
            
        # Filtrage des donn√©es
        filtered_data = active_data[
            active_data['value_key'].isin(selected_events)
        ].copy()
        
        if filtered_data.empty:
            return {'error': 'Aucune donn√©e trouv√©e pour les √©v√©nements s√©lectionn√©s'}
        
        analysis = {}
        type_mapping = self.detect_value_types()
        
        # Analyse par type
        for event in selected_events:
            event_data = filtered_data[filtered_data['value_key'] == event]
            event_type = type_mapping.get(event, 'custom')
            
            if event_type == 'price':
                analysis[event] = self._analyze_price_data(event_data, event)
            elif event_type == 'product':
                analysis[event] = self._analyze_product_data(event_data, event)
            else:
                analysis[event] = self._analyze_custom_data(event_data, event)
                
        return analysis
    
    def _analyze_price_data(self, data: pd.DataFrame, event: str) -> Dict:
        """Analyse sp√©cialis√©e pour les donn√©es de prix"""
        # Filtrer les valeurs num√©riques valides
        price_data = data.dropna(subset=['value_numeric'])
        
        if price_data.empty:
            return {'type': 'price', 'error': 'Aucune donn√©e de prix valide'}
        
        total_revenue = price_data['value_numeric'].sum()
        avg_transaction = price_data['value_numeric'].mean()
        transaction_count = len(price_data)
        
        # √âvolution temporelle du CA - grouper par jour pour √©viter les points multiples
        # Grouper par jour et sommer les revenus
        daily_revenue = price_data.groupby(
            price_data['date'].dt.date
        )['value_numeric'].sum().reset_index()
        daily_revenue.columns = ['date', 'daily_revenue']
        
        # Convertir la date en datetime et trier
        daily_revenue['date'] = pd.to_datetime(daily_revenue['date'])
        daily_revenue = daily_revenue.sort_values('date')
        
        # Calculer le CA cumul√© par jour
        daily_revenue['cumulative_revenue'] = daily_revenue['daily_revenue'].cumsum()
        
        return {
            'type': 'price',
            'total_revenue': total_revenue,
            'avg_transaction': avg_transaction,
            'transaction_count': transaction_count,
            'temporal_data': daily_revenue,
            'distribution': price_data['value_numeric'].value_counts().sort_index()
        }
    
    def _analyze_product_data(self, data: pd.DataFrame, event: str) -> Dict:
        """Analyse sp√©cialis√©e pour les donn√©es produit"""
        product_distribution = data['value_text'].value_counts()
        
        # √âvolution temporelle par produit
        temporal_data = data.groupby(['date', 'value_text']).size().unstack(fill_value=0)
        
        return {
            'type': 'product',
            'distribution': product_distribution,
            'temporal_data': temporal_data,
            'top_products': product_distribution.head(10)
        }
    
    def _analyze_custom_data(self, data: pd.DataFrame, event: str) -> Dict:
        """Analyse g√©n√©rale pour les donn√©es personnalis√©es"""
        distribution = data['value_text'].value_counts()
        
        return {
            'type': 'custom',
            'distribution': distribution,
            'count': len(data)
        }


def configure_date_axis(fig, dates_data):
    """Configure intelligemment l'axe des dates pour √©viter la r√©p√©tition des labels"""
    num_days = len(dates_data) if hasattr(dates_data, '__len__') else len(set(dates_data))
    
    if num_days <= 2:
        # Pour 1-2 jours, afficher les dates disponibles sans r√©p√©tition
        unique_dates = sorted(set(dates_data))
        fig.update_xaxes(
            tickmode='array',
            tickvals=unique_dates,
            ticktext=[pd.to_datetime(d).strftime('%d/%m/%Y') for d in unique_dates],
            tickformat='%d/%m/%Y'
        )
    elif num_days <= 7:
        # Pour une semaine, un tick par jour
        fig.update_xaxes(
            tickformat='%d/%m',
            tickmode='linear',
            dtick='D1'
        )
    else:
        # Pour plus de 7 jours, espacement automatique intelligent
        fig.update_xaxes(
            tickformat='%d/%m',
            tickmode='auto',
            nticks=min(10, num_days)
        )
    
    return fig


def load_and_validate_data(uploaded_file) -> Tuple[Optional[ROIDataProcessor], str]:
    """Charge et valide le fichier upload√©"""
    try:
        # Lecture du fichier CSV
        df = pd.read_csv(uploaded_file)
        
        # Cr√©ation du processeur de donn√©es
        processor = ROIDataProcessor(df)
        
        # Validation de la structure
        is_valid, message = processor.validate_data_structure()
        
        if not is_valid:
            return None, f"‚ùå Erreur de validation: {message}"
        
        return processor, f"‚úÖ Fichier charg√© avec succ√®s: {len(df)} lignes"
        
    except Exception as e:
        return None, f"‚ùå Erreur lors du chargement: {str(e)}"


def display_period_picker(processor: ROIDataProcessor) -> Tuple[datetime, datetime]:
    """Affiche le s√©lecteur de p√©riode et retourne les dates s√©lectionn√©es"""
    # Obtenir la plage de dates disponible dans le fichier
    min_date, max_date = processor.get_date_range()
    
    if pd.isna(min_date) or pd.isna(max_date):
        st.error("‚ùå Impossible de d√©terminer la plage de dates du fichier")
        return None, None
    
    # Convertir en dates Python pour Streamlit
    min_date_py = min_date.date()
    max_date_py = max_date.date()
    
    # Interface de s√©lection simple
    col1, col2 = st.columns(2)
    
    with col1:
        start_date = st.date_input(
            "Date de d√©but",
            value=min_date_py,
            min_value=min_date_py,
            max_value=max_date_py,
            key="start_date"
        )
    
    with col2:
        end_date = st.date_input(
            "Date de fin", 
            value=max_date_py,
            min_value=min_date_py,
            max_value=max_date_py,
            key="end_date"
        )
    
    # Validation de la s√©lection
    if start_date > end_date:
        st.error("‚ùå La date de d√©but doit √™tre ant√©rieure √† la date de fin")
        return None, None
    
    # Conversion en datetime pour le filtrage
    start_datetime = datetime.combine(start_date, datetime.min.time())
    end_datetime = datetime.combine(end_date, datetime.max.time())
    
    return start_datetime, end_datetime


def display_event_explorer(processor: ROIDataProcessor):
    """Affiche l'explorateur d'√©v√©nements (Niveau 2)"""
    st.markdown('<div class="section-header">üîç Explorateur d\'√âv√©nements</div>', 
                unsafe_allow_html=True)
    
    # Utiliser les donn√©es actives (filtr√©es ou compl√®tes)
    active_data = processor.get_active_data()
    
    # Liste des noms d'√©v√©nements disponibles (pas les value_key, mais les 'name')
    available_event_names = sorted(active_data['name'].unique())
    
    st.subheader("S√©lection des √©v√©nements √† analyser")
    
    # Interface de s√©lection par nom d'√©v√©nement
    selected_event_names = []
    
    # Affichage de tous les √©v√©nements avec checkbox
    st.write("**üìã √âv√©nements disponibles**")
    
    # Organisation en colonnes pour une meilleure pr√©sentation
    cols = st.columns(2)
    col_idx = 0
    
    for event_name in available_event_names:
        with cols[col_idx % 2]:
            # Comptage CORRECT du nombre d'√©v√©nements uniques (pas de doublons dus au format long)
            # Compter les combinaisons uniques (name, date, conversation_id)
            event_data = active_data[active_data['name'] == event_name]
            event_count = len(event_data.drop_duplicates(subset=['name', 'date', 'conversation_id']))
            
            if st.checkbox(f"{event_name} ({event_count} √©v√©nements)", key=f"event_{event_name}"):
                selected_event_names.append(event_name)
        
        col_idx += 1
    
    # Analyse automatique des √©v√©nements s√©lectionn√©s (pas besoin de bouton)
    if selected_event_names:
        st.markdown("---")
        display_event_name_analysis(processor, selected_event_names)


def display_event_name_analysis(processor: ROIDataProcessor, selected_event_names: List[str]):
    """Affiche l'analyse des √©v√©nements s√©lectionn√©s par nom"""
    st.markdown('<div class="section-header">üìà Analyse des √âv√©nements S√©lectionn√©s</div>', 
                unsafe_allow_html=True)
    
    # Utiliser les donn√©es actives (filtr√©es ou compl√®tes)
    active_data = processor.get_active_data()
    
    # Filtrage des donn√©es par nom d'√©v√©nement
    filtered_data = active_data[
        active_data['name'].isin(selected_event_names)
    ].copy()
    
    if filtered_data.empty:
        st.warning("Aucune donn√©e trouv√©e pour les √©v√©nements s√©lectionn√©s")
        return
    
    # Conversion des dates si pas d√©j√† fait
    filtered_data['date'] = pd.to_datetime(filtered_data['date'], errors='coerce')
    
    # Normaliser les timezones pour √©viter les erreurs de comparaison
    if filtered_data['date'].dt.tz is not None:
        filtered_data['date'] = filtered_data['date'].dt.tz_convert(None)
    
    # === M√âTRIQUES PRINCIPALES ===
    # Compter les √©v√©nements uniques (√©viter les doublons dus au format long)
    unique_events = filtered_data.drop_duplicates(subset=['name', 'date', 'conversation_id'])
    total_events = len(unique_events)
    unique_days = filtered_data['date'].dt.date.nunique()
    avg_events_per_day = total_events / max(unique_days, 1)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            label="Nombre total d'√©v√©nements",
            value=f"{total_events:,}",
            delta=None
        )
    
    with col2:
        st.metric(
            label="Jours d'activit√©",
            value=unique_days,
            delta=None
        )
    
    with col3:
        st.metric(
            label="√âv√©nements/jour (moy.)",
            value=f"{avg_events_per_day:.1f}",
            delta=None
        )
    
    # === √âVOLUTION TEMPORELLE DES √âV√âNEMENTS ===
    st.subheader("üìä √âvolution du nombre d'√©v√©nements")
    
    # Comptage par jour et par nom d'√©v√©nement (utiliser unique_events pour √©viter doublons)
    daily_events = unique_events.groupby([
        unique_events['date'].dt.date, 'name'
    ]).size().unstack(fill_value=0).reset_index()
    
    # Si un seul type d'√©v√©nement, graphique simple
    if len(selected_event_names) == 1:
        daily_simple = unique_events.groupby(
            unique_events['date'].dt.date
        ).size().reset_index()
        daily_simple.columns = ['date', 'count']
        
        # Convertir la date en datetime pour un meilleur affichage
        daily_simple['date'] = pd.to_datetime(daily_simple['date'])
        
        fig_timeline = px.line(
            daily_simple,
            x='date',
            y='count',
            title=f"√âvolution - {selected_event_names[0]}",
            labels={'count': 'Nombre d\'√©v√©nements', 'date': 'Date'}
        )
        # Configurer l'axe des dates intelligemment
        configure_date_axis(fig_timeline, daily_simple['date'])
        
    else:
        # Plusieurs √©v√©nements : graphique empil√©
        # Convertir la date en datetime pour un meilleur affichage
        daily_events['date'] = pd.to_datetime(daily_events['date'])
        
        daily_melted = daily_events.melt(
            id_vars=['date'], 
            var_name='event_name', 
            value_name='count'
        )
        
        fig_timeline = px.area(
            daily_melted,
            x='date',
            y='count',
            color='event_name',
            title="√âvolution par type d'√©v√©nement",
            labels={'count': 'Nombre d\'√©v√©nements', 'date': 'Date'}
        )
        # Configurer l'axe des dates intelligemment
        configure_date_axis(fig_timeline, daily_events['date'])
    
    st.plotly_chart(fig_timeline, use_container_width=True)
    
    # === D√âTECTION ET ANALYSE DES PRIX ===
    # Chercher les √©v√©nements qui ont des valeurs de prix
    price_data = filtered_data[
        (filtered_data['value_key'].str.contains('price', case=False, na=False)) |
        (filtered_data['value_key'].str.contains('prix', case=False, na=False))
    ].copy()
    
    price_data = price_data.dropna(subset=['value_numeric'])
    
    if not price_data.empty:
        st.markdown("---")
        st.subheader("üí∞ Analyse du Chiffre d'Affaires")
        
        # M√©triques CA
        total_revenue = price_data['value_numeric'].sum()
        transaction_count = len(price_data)
        avg_transaction = price_data['value_numeric'].mean()
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                label="Chiffre d'Affaires Total",
                value=f"{total_revenue:,.2f}‚Ç¨".replace(',', ' '),
                delta=None
            )
        
        with col2:
            st.metric(
                label="Nombre de Transactions",
                value=transaction_count,
                delta=None
            )
        
        with col3:
            st.metric(
                label="Transaction Moyenne",
                value=f"{avg_transaction:,.2f}‚Ç¨".replace(',', ' '),
                delta=None
            )
        
        # √âvolution du CA cumul√©
        st.subheader("üìà √âvolution du Chiffre d'Affaires Cumul√©")
        
        # Grouper les revenus par jour complet pour √©viter les points multiples dans la journ√©e
        daily_revenue = price_data.groupby(
            price_data['date'].dt.date
        )['value_numeric'].sum().reset_index()
        daily_revenue.columns = ['date', 'daily_revenue']
        
        # Convertir la date en datetime pour un meilleur affichage
        daily_revenue['date'] = pd.to_datetime(daily_revenue['date'])
        
        # Calculer le CA cumul√© par jour
        daily_revenue = daily_revenue.sort_values('date')
        daily_revenue['cumulative_revenue'] = daily_revenue['daily_revenue'].cumsum()
        
        fig_revenue = px.line(
            daily_revenue,
            x='date',
            y='cumulative_revenue',
            title="Chiffre d'Affaires Cumul√© par Jour",
            labels={'cumulative_revenue': 'CA Cumul√© (‚Ç¨)', 'date': 'Date'}
        )
        
        # Ajout de markers pour les points de vente quotidiens
        fig_revenue.add_scatter(
            x=daily_revenue['date'],
            y=daily_revenue['cumulative_revenue'].round(2),
            mode='markers',
            name='CA Quotidien',
            marker=dict(size=8, color='red', symbol='circle'),
            hovertemplate='<b>%{x}</b><br>' +
                         'CA cumul√©: ‚Ç¨%{y:,.2f}<br>' +
                         '<extra></extra>'
        )
        
        # Configurer l'axe des dates intelligemment
        configure_date_axis(fig_revenue, daily_revenue['date'])
        
        st.plotly_chart(fig_revenue, use_container_width=True)
        
        # R√©partition des prix si plusieurs valeurs
        if transaction_count > 1:
            st.subheader("üíµ R√©partition des Prix")
            
            # Utiliser un histogramme pour mieux visualiser la distribution avec outliers
            fig_price_dist = px.histogram(
                price_data,
                x='value_numeric',
                title="Distribution des Prix",
                labels={'value_numeric': 'Prix (‚Ç¨)', 'count': 'Nombre de Transactions'},
                nbins=min(25, len(price_data) // 3)  # Adapter le nombre de bins
            )
            
            # Am√©liorer la lisibilit√©
            fig_price_dist.update_layout(
                height=450,
                showlegend=False,
                xaxis=dict(
                    title="Prix (‚Ç¨)",
                    showgrid=True,
                    gridcolor='rgba(255,255,255,0.1)'
                ),
                yaxis=dict(
                    title="Nombre de Transactions",
                    showgrid=True,
                    gridcolor='rgba(255,255,255,0.1)'
                )
            )
            
            st.plotly_chart(fig_price_dist, use_container_width=True)
    
    # === ANALYSE G√âN√âRIQUE DES AUTRES CL√âS-VALEURS ===
    st.markdown("---")
    st.subheader("üìã Analyse des M√©tadonn√©es")
    
    # Identifier toutes les cl√©s-valeurs non-prix
    all_value_keys = filtered_data['value_key'].unique()
    non_price_keys = [
        key for key in all_value_keys 
        if not ('price' in key.lower() or 'prix' in key.lower()) and key != 'no_value'
    ]
    
    if non_price_keys:
        st.write(f"**Analyse d√©taill√©e des {len(non_price_keys)} types de m√©tadonn√©es d√©tect√©es**")
        
        # Cr√©er des onglets ou colonnes selon le nombre de cl√©s
        if len(non_price_keys) <= 3:
            # Si 3 cl√©s ou moins, utiliser des colonnes
            cols = st.columns(len(non_price_keys))
            
            for idx, value_key in enumerate(sorted(non_price_keys)):
                with cols[idx]:
                    display_value_key_analysis(filtered_data, value_key)
        else:
            # Si plus de 3 cl√©s, utiliser des onglets
            tabs = st.tabs([f"üìä {key.title()}" for key in sorted(non_price_keys)])
            
            for idx, value_key in enumerate(sorted(non_price_keys)):
                with tabs[idx]:
                    display_value_key_analysis(filtered_data, value_key)
    else:
        st.info("Aucune m√©tadonn√©e non-prix d√©tect√©e dans les √©v√©nements s√©lectionn√©s")
    
    


def display_value_key_analysis(filtered_data: pd.DataFrame, value_key: str):
    """Affiche l'analyse d√©taill√©e d'une cl√©-valeur sp√©cifique"""
    # Filtrer les donn√©es pour cette cl√©-valeur
    key_data = filtered_data[filtered_data['value_key'] == value_key].copy()
    
    if key_data.empty:
        st.warning(f"Aucune donn√©e trouv√©e pour la cl√© '{value_key}'")
        return
    
    # Compter les occurrences de chaque valeur
    value_counts = key_data['value_text'].value_counts()
    total_count = len(key_data)
    
    # Calculer les prix associ√©s pour chaque valeur de m√©tadonn√©e
    # On va chercher les prix dans les √©v√©nements de m√™me conversation_id, name et date
    price_by_metadata = {}
    has_associated_prices = False
    
    for value in value_counts.index:
        # R√©cup√©rer les √©v√©nements avec cette valeur de m√©tadonn√©e
        metadata_events = key_data[key_data['value_text'] == value]
        total_revenue = 0
        
        # Pour chaque √©v√©nement avec cette m√©tadonn√©e, chercher les prix associ√©s
        for _, event in metadata_events.iterrows():
            # Chercher les prix dans la m√™me conversation/date/nom
            matching_prices = filtered_data[
                (filtered_data['name'] == event['name']) &
                (filtered_data['date'] == event['date']) &
                (filtered_data['conversation_id'] == event['conversation_id']) &
                (filtered_data['value_key'].str.contains('price', case=False, na=False)) &
                (filtered_data['value_numeric'].notna())
            ]
            
            if not matching_prices.empty:
                total_revenue += matching_prices['value_numeric'].sum()
                has_associated_prices = True
        
        price_by_metadata[value] = total_revenue
    
    # Cr√©er le DataFrame avec occurrences, pourcentages et prix
    analysis_data = []
    total_revenue_all = sum(price_by_metadata.values()) if has_associated_prices else 0
    
    for value, count in value_counts.items():
        percentage = (count / total_count) * 100
        revenue = price_by_metadata.get(value, 0)
        
        row_data = {
            'Valeur': value if value else '(vide)',
            'Occurrences': count,
            'Pourcentage': f"{percentage:.1f}%"
        }
        
        # Ajouter les colonnes de prix si applicable
        if has_associated_prices:
            row_data['CA Associ√© (‚Ç¨)'] = f"{revenue:,.2f}".replace(',', ' ')
            if total_revenue_all > 0:
                revenue_percentage = (revenue / total_revenue_all) * 100
                row_data['% du CA'] = f"{revenue_percentage:.1f}%"
        
        analysis_data.append(row_data)
    
    analysis_df = pd.DataFrame(analysis_data)
    
    # Affichage du titre et des m√©triques
    st.write(f"**{value_key.title().replace('_', ' ')}**")
    
    # M√©triques principales
    if has_associated_prices:
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Valeurs uniques", len(value_counts))
        with col2:
            st.metric("Total occurrences", total_count)
        with col3:
            st.metric("CA Total", f"{total_revenue_all:,.2f}‚Ç¨".replace(',', ' '))
        with col4:
            avg_revenue = total_revenue_all / total_count if total_count > 0 else 0
            st.metric("CA Moyen", f"{avg_revenue:,.2f}‚Ç¨".replace(',', ' '))
    else:
        col1, col2 = st.columns(2)
        with col1:
            st.metric("Valeurs uniques", len(value_counts))
        with col2:
            st.metric("Total occurrences", total_count)
    
    # Tableau des r√©sultats
    st.dataframe(
        analysis_df,
        use_container_width=True,
        hide_index=True
    )
    
    # Top des valeurs par CA si applicable
    if has_associated_prices and total_revenue_all > 0 and len(value_counts) > 1:
        st.write("üí∞ **Top par Chiffre d'Affaires:**")
        
        # Cr√©er un ranking par CA
        revenue_ranking = sorted(
            [(value, price_by_metadata.get(value, 0)) for value in value_counts.index],
            key=lambda x: x[1],
            reverse=True
        )
        
        ranking_col1, ranking_col2 = st.columns(2)
        
        with ranking_col1:
            st.write("**üèÜ Top 3 CA:**")
            medals = ["ü•á", "ü•à", "ü•â"]
            for i, (value, revenue) in enumerate(revenue_ranking[:3]):
                count = value_counts[value]
                medal = medals[i] if i < len(medals) else "‚Ä¢"
                st.write(f"{medal} **{value}**: {revenue:,.2f}‚Ç¨ ({count} occurrences)".replace(',', ' '))
        
        with ranking_col2:
            if len(revenue_ranking) > 1:
                st.write("**üìà Panier moyen par valeur:**")
                # Container avec scroll pour les longues listes
                with st.container(height=200):
                    for value, revenue in revenue_ranking:
                        count = value_counts[value]
                        avg_basket = revenue / count if count > 0 else 0
                        st.write(f"‚Ä¢ **{value}**: {avg_basket:,.2f}‚Ç¨/transaction".replace(',', ' '))
    


def display_detailed_analysis(processor: ROIDataProcessor, selected_events: List[str]):
    """Affiche l'analyse d√©taill√©e des √©v√©nements s√©lectionn√©s"""
    st.markdown('<div class="section-header">üìà Analyse D√©taill√©e</div>', 
                unsafe_allow_html=True)
    
    analysis_results = processor.analyze_selected_events(selected_events)
    
    if 'error' in analysis_results:
        st.error(analysis_results['error'])
        return
    
    # Calcul des totaux pour le mode revenus
    total_revenue = 0
    total_transactions = 0
    revenue_events = []
    
    for event, analysis in analysis_results.items():
        if analysis.get('type') == 'price' and 'total_revenue' in analysis:
            total_revenue += analysis['total_revenue']
            total_transactions += analysis['transaction_count']
            revenue_events.append(event)
    
    # Affichage des m√©triques principales si mode revenus activ√©
    if total_revenue > 0:
        st.subheader("üí∞ Mode Chiffre d'Affaires Activ√©")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(
                label="Chiffre d'Affaires Total",
                value=f"{total_revenue:,.2f}‚Ç¨".replace(',', ' '),
                delta=None
            )
        with col2:
            st.metric(
                label="Nombre de Transactions",
                value=total_transactions,
                delta=None
            )
        with col3:
            avg_transaction = total_revenue / total_transactions if total_transactions > 0 else 0
            st.metric(
                label="Transaction Moyenne",
                value=f"{avg_transaction:,.2f}‚Ç¨".replace(',', ' '),
                delta=None
            )
    
    # Analyse d√©taill√©e par √©v√©nement
    for event, analysis in analysis_results.items():
        with st.expander(f"üìä Analyse de '{event}' (Type: {analysis.get('type', 'unknown')})"):
            
            if analysis.get('type') == 'price':
                display_price_analysis(event, analysis)
            elif analysis.get('type') == 'product':
                display_product_analysis(event, analysis)
            else:
                display_custom_analysis(event, analysis)


def display_price_analysis(event: str, analysis: Dict):
    """Affiche l'analyse sp√©cialis√©e pour les prix"""
    if 'error' in analysis:
        st.error(analysis['error'])
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**R√©partition des prix**")
        if 'distribution' in analysis and 'temporal_data' in analysis:
            # Utiliser les donn√©es originales pour l'histogramme
            temporal_data = analysis['temporal_data']
            
            # V√©rifier s'il y a une colonne avec les prix individuels
            if 'daily_revenue' in temporal_data.columns:
                # Si on a les donn√©es agr√©g√©es par jour, on ne peut pas faire un histogramme d√©taill√©
                # On utilisera alors la distribution existante
                price_dist_df = pd.DataFrame({
                    'prix': analysis['distribution'].index,
                    'frequence': analysis['distribution'].values
                }).sort_values('prix')
                
                fig_price_dist = px.histogram(
                    x=price_dist_df['prix'].repeat(price_dist_df['frequence']),
                    title=f"Distribution des prix - {event}",
                    labels={'x': 'Prix (‚Ç¨)', 'count': 'Fr√©quence'},
                    nbins=min(20, len(price_dist_df))
                )
            else:
                # Utiliser directement les donn√©es si disponibles
                fig_price_dist = px.histogram(
                    analysis['distribution'].index.repeat(analysis['distribution'].values),
                    title=f"Distribution des prix - {event}",
                    labels={'x': 'Prix (‚Ç¨)', 'count': 'Fr√©quence'},
                    nbins=min(20, len(analysis['distribution']))
                )
            
            # Am√©liorer la lisibilit√©
            fig_price_dist.update_layout(
                height=400,
                showlegend=False,
                xaxis=dict(
                    title="Prix (‚Ç¨)",
                    showgrid=True,
                    gridcolor='rgba(255,255,255,0.1)'
                ),
                yaxis=dict(
                    title="Fr√©quence",
                    showgrid=True,
                    gridcolor='rgba(255,255,255,0.1)'
                )
            )
            
            st.plotly_chart(fig_price_dist, use_container_width=True)
    
    with col2:
        st.write("**√âvolution du CA cumul√©**")
        if 'temporal_data' in analysis:
            temporal_data = analysis['temporal_data']
            fig_cumul = px.line(
                temporal_data,
                x='date',
                y='cumulative_revenue',
                title=f"CA cumul√© - {event}",
                labels={'cumulative_revenue': 'CA Cumul√© (‚Ç¨)', 'date': 'Date'}
            )
            # Configurer l'axe des dates intelligemment
            configure_date_axis(fig_cumul, temporal_data['date'])
            st.plotly_chart(fig_cumul, use_container_width=True)


def display_product_analysis(event: str, analysis: Dict):
    """Affiche l'analyse sp√©cialis√©e pour les produits"""
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("**R√©partition des produits**")
        if 'distribution' in analysis:
            fig_product_dist = px.pie(
                values=analysis['distribution'].values,
                names=analysis['distribution'].index,
                title=f"R√©partition des produits - {event}"
            )
            fig_product_dist.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig_product_dist, use_container_width=True)
    
    with col2:
        st.write("**Top produits**")
        if 'top_products' in analysis:
            top_products = analysis['top_products']
            fig_top = px.bar(
                x=top_products.values,
                y=top_products.index,
                orientation='h',
                title=f"Top produits - {event}",
                labels={'x': 'Nombre de ventes', 'y': 'Produit'}
            )
            st.plotly_chart(fig_top, use_container_width=True)


def display_custom_analysis(event: str, analysis: Dict):
    """Affiche l'analyse pour les donn√©es personnalis√©es"""
    st.write("**R√©partition des valeurs**")
    if 'distribution' in analysis:
        fig_custom = px.bar(
            x=analysis['distribution'].values,
            y=analysis['distribution'].index,
            orientation='h',
            title=f"Distribution - {event}",
            labels={'x': 'Fr√©quence', 'y': 'Valeur'}
        )
        st.plotly_chart(fig_custom, use_container_width=True)


def analyze_conversion_performance(processor: ROIDataProcessor, conversion_events) -> Dict:
    """Analyse compl√®te de la performance des √©v√©nements de conversion s√©lectionn√©s"""
    
    # Gestion de liste ou d'un seul √©v√©nement
    events_list = conversion_events if isinstance(conversion_events, list) else [conversion_events]
    
    active_data = processor.get_active_data()
    
    # Conversion des dates
    active_data['date'] = pd.to_datetime(active_data['date'], errors='coerce')
    if active_data['date'].dt.tz is not None:
        active_data['date'] = active_data['date'].dt.tz_convert(None)
    
    # 1. Analyser tous les √©v√©nements de conversion s√©lectionn√©s
    conversion_events_data = active_data[active_data['name'].isin(events_list)].copy()
    
    if conversion_events_data.empty:
        events_str = "', '".join(events_list)
        return {'error': f"Aucun √©v√©nement trouv√© parmi '{events_str}'"}
    
    # 2. D√©tecter s'il y a des prix directement dans les √©v√©nements de conversion
    conversion_with_prices = conversion_events_data[
        (conversion_events_data['value_key'].str.contains('price', case=False, na=False) | 
         conversion_events_data['value_key'].str.contains('prix', case=False, na=False)) &
        (conversion_events_data['value_numeric'].notna())
    ]
    
    # 3. Compter les conversations uniques
    unique_conversations = conversion_events_data['conversation_id'].unique()
    valid_conversations = [
        conv_id for conv_id in unique_conversations 
        if not (pd.isna(conv_id) or conv_id == '')
    ]
    
    result = {
        'conversion_events': events_list,
        'total_conversion_events': len(conversion_events_data),
        'unique_conversations': len(valid_conversations),
        'has_direct_prices': not conversion_with_prices.empty,
        'analysis_mode': 'direct_prices' if not conversion_with_prices.empty else 'price_tracking'
    }
    
    # 4. Analyse selon le mode d√©tect√©
    if result['analysis_mode'] == 'direct_prices':
        # Mode prix directs
        total_revenue = conversion_with_prices['value_numeric'].sum()
        avg_price = conversion_with_prices['value_numeric'].mean()
        transaction_count = len(conversion_with_prices)
        
        # √âvolution temporelle
        daily_revenue = conversion_with_prices.groupby(
            conversion_with_prices['date'].dt.date
        )['value_numeric'].sum().reset_index()
        daily_revenue.columns = ['date', 'daily_revenue']
        daily_revenue['date'] = pd.to_datetime(daily_revenue['date'])
        daily_revenue = daily_revenue.sort_values('date')
        daily_revenue['cumulative_revenue'] = daily_revenue['daily_revenue'].cumsum()
        
        result.update({
            'total_revenue': total_revenue,
            'average_price': avg_price,
            'transaction_count': transaction_count,
            'temporal_data': daily_revenue
        })
        
    else:
        # Mode tracking du dernier prix
        tracking_result = analyze_conversion_tracking(processor, events_list)
        if 'error' not in tracking_result:
            # Cr√©er les donn√©es temporelles √† partir des r√©sultats de tracking
            temporal_data = tracking_result['analysis_data'].groupby(
                tracking_result['analysis_data']['conversion_date'].dt.date
            ).agg({
                'last_price': 'sum',
                'conversation_id': 'count'
            }).reset_index()
            temporal_data.columns = ['date', 'daily_revenue', 'daily_conversions']
            
            # Calcul cumulatif
            temporal_data['date'] = pd.to_datetime(temporal_data['date'])
            temporal_data = temporal_data.sort_values('date')
            temporal_data['cumulative_revenue'] = temporal_data['daily_revenue'].cumsum()
            
            result.update({
                'total_revenue': tracking_result['total_revenue'],
                'average_price': tracking_result['average_price'], 
                'successful_conversions': tracking_result['successful_conversions'],
                'temporal_data': temporal_data
            })
        else:
            result.update({
                'total_revenue': 0,
                'average_price': 0,
                'successful_conversions': 0,
                'tracking_error': tracking_result['error']
            })
    
    return result


def analyze_conversion_tracking(processor: ROIDataProcessor, conversion_events) -> Dict:
    """Analyse le tracking de conversion en trouvant le dernier prix avant les √©v√©nements de conversion"""
    
    # Gestion de liste ou d'un seul √©v√©nement
    events_list = conversion_events if isinstance(conversion_events, list) else [conversion_events]
    
    # Utiliser les donn√©es actives (filtr√©es ou compl√®tes)
    active_data = processor.get_active_data()
    
    # Conversion des dates
    active_data['date'] = pd.to_datetime(active_data['date'], errors='coerce')
    
    # Normaliser les timezones pour √©viter les erreurs de comparaison
    if active_data['date'].dt.tz is not None:
        active_data['date'] = active_data['date'].dt.tz_convert(None)
    
    # 1. Trouver toutes les conversations qui contiennent au moins un des √©v√©nements de conversion
    conversations_with_conversion = active_data[
        active_data['name'].isin(events_list)
    ]['conversation_id'].unique()
    
    if len(conversations_with_conversion) == 0:
        events_str = "', '".join(events_list)
        return {'error': f"Aucune conversation trouv√©e avec les √©v√©nements '{events_str}'"}
    
    conversion_analysis = []
    total_revenue = 0
    
    for conv_id in conversations_with_conversion:
        if pd.isna(conv_id) or conv_id == '':
            continue
            
        # 2. R√©cup√©rer tous les √©v√©nements de cette conversation, tri√©s par date
        conv_events = active_data[
            active_data['conversation_id'] == conv_id
        ].sort_values('date')
        
        # 3. Trouver la premi√®re occurrence d'un des √©v√©nements de conversion
        conversion_events_in_conv = conv_events[conv_events['name'].isin(events_list)]
        if conversion_events_in_conv.empty:
            continue
            
        first_conversion = conversion_events_in_conv.iloc[0]
        first_conversion_date = first_conversion['date']
        first_conversion_event = first_conversion['name']
        
        # 4. Chercher tous les √©v√©nements avec prix avant cette date de conversion
        price_events_before = conv_events[
            (conv_events['date'] < first_conversion_date) &
            (conv_events['value_key'].str.contains('price', case=False, na=False) | 
             conv_events['value_key'].str.contains('prix', case=False, na=False)) &
            (conv_events['value_numeric'].notna())
        ]
        
        # 5. Prendre le dernier prix (le plus proche de la conversion)
        if not price_events_before.empty:
            last_price_event = price_events_before.iloc[-1]  # Le dernier chronologiquement
            last_price = last_price_event['value_numeric']
            
            conversion_analysis.append({
                'conversation_id': conv_id,
                'conversion_date': first_conversion_date,
                'conversion_event': first_conversion_event,
                'last_price': last_price,
                'last_price_date': last_price_event['date'],
                'last_price_event': last_price_event['name'],
                'time_to_conversion': (first_conversion_date - last_price_event['date']).total_seconds() / 60  # en minutes
            })
            
            total_revenue += last_price
    
    # Cr√©ation du DataFrame pour l'analyse
    if not conversion_analysis:
        events_str = "', '".join(events_list)
        return {'error': f"Aucun prix trouv√© avant les √©v√©nements de conversion '{events_str}'"}
    
    analysis_df = pd.DataFrame(conversion_analysis)
    
    # Tri par date de conversion pour l'√©volution cumulative
    analysis_df = analysis_df.sort_values('conversion_date')
    analysis_df['cumulative_revenue'] = analysis_df['last_price'].cumsum()
    
    return {
        'success': True,
        'conversion_events': events_list,
        'total_conversations': len(conversations_with_conversion),
        'successful_conversions': len(analysis_df),
        'total_revenue': total_revenue,
        'average_price': analysis_df['last_price'].mean(),
        'analysis_data': analysis_df,
        'price_distribution': analysis_df['last_price'].value_counts().sort_index(),
        'avg_time_to_conversion': analysis_df['time_to_conversion'].mean()
    }


def display_conversion_performance_section(processor: ROIDataProcessor, conversion_events):
    """Affiche la section de performance de conversion (prioritaire apr√®s configuration)"""
    
    if not conversion_events:
        return
    
    # Gestion de liste ou d'un seul √©v√©nement
    events_list = conversion_events if isinstance(conversion_events, list) else [conversion_events]
    
    st.markdown('<div class="section-header">üéØ Indicateurs de Performance de Conversion</div>', 
                unsafe_allow_html=True)
    
    with st.spinner("Analyse des performances de conversion..."):
        performance = analyze_conversion_performance(processor, events_list)
    
    if 'error' in performance:
        st.error(f"‚ùå {performance['error']}")
        return
    
    # === BADGE MODE D'ANALYSE ===
    mode_text = "üìä Prix Directs" if performance['analysis_mode'] == 'direct_prices' else "üîç Tracking du Dernier Prix"
    events_text = ", ".join([f"'{e}'" for e in events_list])
    st.info(f"**Mode d'analyse d√©tect√©** : {mode_text} | **√âv√©nements analys√©s** : {events_text}")
    
    # === M√âTRIQUES PRINCIPALES ===
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            label="Conversions",
            value=f"{performance['total_conversion_events']:,}",
            delta=f"sur {performance['unique_conversations']} conversations uniques" if performance['total_conversion_events'] > performance['unique_conversations'] else f"{performance['unique_conversations']} conversations uniques"
        )
    
    with col2:
        if performance.get('total_revenue', 0) > 0:
            st.metric(
                label="Chiffre d'Affaires Total",
                value=f"{performance['total_revenue']:,.2f}‚Ç¨".replace(',', ' '),
                delta=None
            )
        else:
            if 'tracking_error' in performance:
                st.metric(
                    label="Chiffre d'Affaires",
                    value="Non disponible",
                    delta="Aucun prix identifiable"
                )
            else:
                st.metric(
                    label="Chiffre d'Affaires Total",
                    value="0‚Ç¨",
                    delta=None
                )
    
    with col3:
        if performance.get('average_price', 0) > 0:
            st.metric(
                label="Panier Moyen",
                value=f"{performance['average_price']:,.2f}‚Ç¨".replace(',', ' '),
                delta=None
            )
        else:
            st.metric(
                label="Panier Moyen",
                value="N/A",
                delta=None
            )
    
    # === GRAPHIQUE D'√âVOLUTION ===
    if 'temporal_data' in performance and not performance['temporal_data'].empty:
        temporal_data = performance['temporal_data']
        
        # Graphique principal : √©volution cumulative
        fig_evolution = go.Figure()
        
        # Ligne cumulative
        fig_evolution.add_trace(go.Scatter(
            x=temporal_data['date'],
            y=temporal_data['cumulative_revenue'].round(2),
            mode='lines+markers',
            name='CA Cumul√©',
            line=dict(color='#1f77b4', width=3),
            marker=dict(size=8, color='#1f77b4'),
            hovertemplate='<b>%{x}</b><br>' +
                         'CA cumul√©: ‚Ç¨%{y:,.2f}<br>' +
                         '<extra></extra>'
        ))
        
        # Points quotidiens
        fig_evolution.add_trace(go.Scatter(
            x=temporal_data['date'],
            y=temporal_data['daily_revenue'].round(2),
            mode='markers',
            name='CA quotidien',
            marker=dict(size=10, color='red', symbol='diamond'),
            hovertemplate='<b>%{x}</b><br>' +
                         'CA du jour: ‚Ç¨%{y:,.2f}<br>' +
                         '<extra></extra>'
        ))
        
        events_title = ", ".join(events_list)
        fig_evolution.update_layout(
            title=f"Performance de Conversion - {events_title}",
            xaxis_title="Date",
            yaxis_title="Montant (‚Ç¨)",
            hovermode='x unified',
            height=400
        )
        
        # Configurer l'axe des dates intelligemment
        configure_date_axis(fig_evolution, temporal_data['date'])
        
        st.plotly_chart(fig_evolution, use_container_width=True)
    
    # === INFORMATIONS COMPL√âMENTAIRES ===
    if performance['analysis_mode'] == 'price_tracking' and 'tracking_error' in performance:
        st.warning(f"‚ö†Ô∏è {performance['tracking_error']}")
        st.info("üí° Cet √©v√©nement de conversion ne contient pas de prix et aucun prix n'a pu √™tre identifi√© dans les √©v√©nements pr√©c√©dents des conversations.")


def main():
    """Fonction principale de l'application"""
    
    # Initialisation des variables de session
    if 'processor' not in st.session_state:
        st.session_state.processor = None
    if 'show_analysis' not in st.session_state:
        st.session_state.show_analysis = False
    
    # Titre principal
    st.title("üìä ROI Tracker")
    
    # Zone d'upload
    st.subheader("üìÅ Upload de fichier")
    uploaded_file = st.file_uploader(
        "Glissez-d√©posez votre fichier CSV ou cliquez pour le s√©lectionner",
        type=['csv'],
        help="Le fichier doit contenir: name, date + soit 'values' (format cl√©:valeur) soit des colonnes 'value_*'"
    )
    
    if uploaded_file is not None:
        # Chargement et validation des donn√©es
        processor, message = load_and_validate_data(uploaded_file)
        
        if processor is not None:
            st.success(message)
            st.session_state.processor = processor
            
            # S√©lection de la p√©riode directement sous l'upload
            start_date, end_date = display_period_picker(processor)
            
            if start_date is not None and end_date is not None:
                # Appliquer le filtre de date
                processor.apply_date_filter(start_date, end_date)
                
                # === CONFIGURATION GENII ===
                # Multi-select pour les √©v√©nements de conversion finale
                active_data = processor.get_active_data()
                available_events = sorted(active_data['name'].unique()) if not active_data.empty else []
                
                if available_events:
                    genii_conversion_events = st.multiselect(
                        "√âv√©nement(s) de conversion finale",
                        options=available_events,
                        default=[],
                        key="genii_conversion_events",
                        help="S√©lectionnez un ou plusieurs √©v√©nements qui marquent une conversion r√©ussie"
                    )
                else:
                    genii_conversion_events = []
                    st.warning("Aucun √©v√©nement disponible dans les donn√©es")
                
                # S√©parateur apr√®s configuration
                st.markdown("---")
                
                # === NOUVELLE SECTION : PERFORMANCE DE CONVERSION ===
                if genii_conversion_events:
                    display_conversion_performance_section(
                        processor,
                        genii_conversion_events
                    )
                    
                    # S√©parateur avant explorateur
                    st.markdown("---")
                
                # Affichage de l'explorateur d'√©v√©nements
                display_event_explorer(processor)
            else:
                st.warning("‚ö†Ô∏è Veuillez s√©lectionner une p√©riode valide pour continuer l'analyse")
            
        else:
            st.error(message)


if __name__ == "__main__":
    main()
